# ðŸ§  General-Purpose Abstractive Speech Summarizer

This repository contains our graduation project based on the work presented in the Interspeech 2024 paper:
[Prompting Large Language Models with Audio for General-Purpose Speech Summarization](https://arxiv.org/abs/2406.05968) by Wonjune Kang and Deb Roy.

## ðŸ“Œ Project Overview
We extend the baseline system,which is an End-to-End system that we can directly prompt a Large Language Model (LLM) using speech instead of traditional ASR, to better support batch training and enable multilingual summarization via a more versatile cost effective audio encoder.

## ðŸ” What the Original Work Did
The original model, implemented in PyTorch, uses a pre-trained HuBERT audio encoder with a lightweight LLM (MiniChat-3B) to:

- Allow direct audio prompts to the LLM (no ASR step needed)

- Perform speech summarization and general-purpose speech prompting

- Support interleaved audio + text prompts

> ðŸ”— Demo: https://llm-speech-summarization.github.io/ \
ðŸ“„ Paper DOI: 10.21437/Interspeech.2024-2213

## ðŸ”§ Our Contributions
In this forked version, we build upon the original pipeline to better serve our goals for general-purpose summarization in multilingual contexts and more efficient training.
Our modifications include:

### âœ… 1. Audio Encoder: mHuBERT-147
We replaced the baseline HuBERT Large encoder with mHuBERT-147 to support multilingual input to support languages other than English (specifically Arabic) and test the use of a more cost-effecitve option.

### âœ… 2. Support for Batch Training
The original training pipeline only supports batch size = 1. We modified parts of the training code and collators to:

- Enable training with batch sizes > 1 to some extent

- Speed up experimentation while preserving the loss structure and sequence alignment

> âš ï¸ Full support for dynamic batching across all modules is still an open area of improvement.

### âœ… 3. Interactive Summarization Tool
We developed a user-friendly web-based tool to highlight the capabilities of the model. It:

- Accepts audio files or live audio input

- Displays LLM-generated abstractive summaries

- Supports chat-style interaction with summaries

ðŸ”— Try the tool here: [Speech Summarization Web App Repository](https://github.com/OmarTarekAbdelWahab/Abstractive-Speech-to-Text-Summarization)

## ðŸ“Š Results
Due to limited access to high-performance hardware within the graduation project timeline, we were only able to train and evaluate our system on the LibriSpeech dataset for English speech inputs.

However, to assess the general performance of our summarization approach, we also measured BERTScore using summaries generated by our LLM pipeline (MiniChat-3B with mHuBERT-147 audio encoder) and compared them to reference summaries from the CNN/DailyMail dataset.

- ðŸ“ˆ BERTScore (F1): 82.5
â†³ Measured on summaries produced from CNN/DailyMail articles using LLaMA 2-Chat 7B as a reference model

- âœ… Summaries were generally cohesive and abstractive, even without fine-tuning on news domain data

- ðŸ§ª Model was only trained on LibriSpeech-960h, but showed promising generalization for English inputs

> ðŸ”¬ Future work could include training on multilingual corpora and evaluating across broader domains and languages.

### Hyperparameter Tuning experiments Results
<img width="1043" height="611" alt="image" src="https://github.com/user-attachments/assets/d797a5f5-f7f1-40ae-b6bb-9effa5436154" />

> red row represents hyperparamters values and BERTscore for baseline model \
> *: this value is the measured value on the filtered test set we could acquire


## ðŸ“š Citation and References
If you find this repository or the original model helpful, please cite the original work:

```
@inproceedings{kang24d_interspeech,
  title     = {Prompting Large Language Models with Audio for General-Purpose Speech Summarization},
  author    = {Wonjune Kang and Deb Roy},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {1955--1959},
  doi       = {10.21437/Interspeech.2024-2213},
}
```
We also acknowledge the following resources and repositories:

- Original repo: https://github.com/wonjune-kang/llm-speech-summarization 

- ðŸ¤— [GeneZC/MiniChat-3B](https://huggingface.co/GeneZC/MiniChat-3B) for the pre-trained MiniChat-3B model

- ðŸ“¦ [MiniMA logit distillation loss](https://github.com/GeneZC/MiniMA) for the implementation of the soft cross-entropy in our logit distillation loss

- ðŸŽ§ [utter-project/mHuBERT-147](https://huggingface.co/utter-project/mHuBERT-147) for the initial weights of the multilingual HuBERT audio encoder

- ðŸŽ§ [facebook/hubert-large-ls960-ft](https://huggingface.co/facebook/hubert-large-ls960-ft) for the initial weights of the HuBERT-based audio encoder

- https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py for the core model structure of the LLM in ```model/audio_llama.py```




